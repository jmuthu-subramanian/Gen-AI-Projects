{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaacafc1",
   "metadata": {
    "id": "aaacafc1"
   },
   "source": [
    "# 🔧 Fine-Tuning a Small LLM with QLoRA\n",
    "\n",
    "**Objective**: Fine-tune a small open-source LLM using QLoRA (Quantized Low-Rank Adaptation) on a sample dataset.\n",
    "\n",
    "This notebook uses `TinyLlama-1.1B-Chat` model and the Alpaca dataset as an example. You can modify the dataset or model as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LnUUN4fxrYse",
   "metadata": {
    "id": "LnUUN4fxrYse"
   },
   "source": [
    "### 🧩 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V38FojrergKR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V38FojrergKR",
    "outputId": "1597f251-1399-494b-b9b9-0e76758e0b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q \\\n",
    "  transformers \\\n",
    "  datasets \\\n",
    "  peft \\\n",
    "  accelerate \\\n",
    "  bitsandbytes \\\n",
    "  trl \\\n",
    "  fsspec==2023.6.0 \\\n",
    "  multiprocess \\\n",
    "  dill \\\n",
    "  xxhash \\\n",
    "  evaluate \\\n",
    "  --no-deps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cYEHzEZVsnD9",
   "metadata": {
    "id": "cYEHzEZVsnD9"
   },
   "source": [
    "### 📚 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EkfEDG5Qs3VI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393,
     "referenced_widgets": [
      "b38b9e8b9d4142ac83757779d2bc6b0d",
      "c272d4568f354cadbebc94266f4ef4f4",
      "9d836c0bc066412cbcd24d0ae25c85cf",
      "7d92d12b77cd429ba29511283c27fc13",
      "fc1cfb35c2ea42d7b5df828c466161a9",
      "00459846851c4c388467e7bab1c63599",
      "1ad4bd8ce7314f27abb70a8669934d4e",
      "a9f06465dc864f96a754ff839eb3c051",
      "5b24b61fa34c4db5a20fb7438846fb57",
      "90587dc17ee9446d9151958a87947c05",
      "49825da536564c8aa11228a9ef0f27cc",
      "12f707c5860b4106993962e022522b86",
      "cbffea7fd27a4622883940817c9e3433",
      "4ccbdc0227554296a31ea0b356449853",
      "52c54b5aa9614963af44a22ce3e1232f",
      "842d577629aa442d996be4e235ec2941",
      "a3d03ae4f31d4fdeb8f252eed758746e",
      "61785cc4d81f46e88f823a2b8f2a45d1",
      "612a889440054a0daaa2c3e76a46e230",
      "9a544502da2b443a987ef662b24d3045",
      "5e4eabb02a3c4ba4a82e92d0f09a5954",
      "6507b48b24b44543b6e9dd4204b37a38",
      "1c6b380e691b4a2ca1e204acb28553ba",
      "ec6a7dd46d4744ccb36055c3423f3c12",
      "49d1ef8394c54d9f965b5cffa30f18f5",
      "b0137161c07d45e494327a2e93ddb4a4",
      "8f2bf116b81f40b0bb44162ece6ede8e",
      "8224ba33b75f485bb5cec02df1b0076e",
      "6b0ec22b27b148da80f234f9526b1843",
      "108166d12bc04042b9475ebc02008a7a",
      "1126ab0f083c439d89464865a4a4b912",
      "a42ad87f7a2844949c4fca74b029c9de",
      "c5513fff14bd4a31a95bd871d8425212"
     ]
    },
    "id": "EkfEDG5Qs3VI",
    "outputId": "c16017be-1418-44b7-b2cf-5a589c4d15f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38b9e8b9d4142ac83757779d2bc6b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f707c5860b4106993962e022522b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6b380e691b4a2ca1e204acb28553ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
       " 'input': '',\n",
       " 'instruction': 'Give three tips for staying healthy.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the community-cleaned version of Alpaca dataset and select a small subset for quick fine-tuning\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "train_data = dataset[\"train\"].select(range(100))\n",
    "train_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X8j_6H9EtA5_",
   "metadata": {
    "id": "X8j_6H9EtA5_"
   },
   "source": [
    "### 🔠 3: Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H5GcjPFBtMbV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1e3d3ba4992641d5bb03b1dbb56c2db4",
      "976cc2c5e3ea418293db3b1ff4996962",
      "8503b5a310e449ca875c0fae1da2a27c",
      "70a45474907e48669c4da7ce295ec068",
      "31e1510f78fb42e9a5e949cc4f2e3a3c",
      "685421d9eb5541558d1b93d66e4f5e82",
      "ae0603b7f10c40d592a0cdace1844a35",
      "98bff1ed01064c89a97d1eedfe162413",
      "51830cde1a1446bdbc1f162c7a76990c",
      "a859feb40bc040d9b12160616eb8eb5c",
      "5c18c270ddf74a8c8b14538b7191df85",
      "d76e2cb6faf94eabbf81cff20e943090",
      "71cf0c1ae7b344c9bda2a1330ae9258b",
      "d8934318c7d144b6ba7f11e30973a81d",
      "b5dafac6c25e48b1af8d70bbb94a1baa",
      "863bddd8e8984f0eaf6a36df1a4bc0bb",
      "0c34b0e7d93c4c94baa0646308b94b43",
      "099c4125c2b344738aef665c76d74e35",
      "41733608b1734006a216fac3ff72dcaf",
      "fd5024d80a6540f8b1c456c15e3e3a5b",
      "afbb1d1c729947a0b7ee4693e3af992c",
      "4b6a1b36899b4122b358c0cea9de57bb",
      "79fab3c11cfd4be595ede3309ffab44c",
      "b37123ca39d04a6a91b9320bffbeb125",
      "ea14f57d86f1488f9f923d6be20ec1e0",
      "b70f239a58394c3e8697cc2f2afda36d",
      "510082467dc144fba3cd7f95a7cdca24",
      "40bef5c2fb6e43bbbf9ad3c865304bc8",
      "931a614aacbb427ea86b9a245ffe6658",
      "894295a17c2841b183dd0b9b7c8769c2",
      "4634b2bb373d414cae9b01e8b2571c69",
      "b4f711716616426b99897dbe90fa2d74",
      "162b76fed17a4e5b82469bfaae5aa300",
      "b35b992a176541eb91bdae8d28fe56a1",
      "f73451d41a814bbda4d324d75d268f31",
      "5fd1cf46565b481faf18eadbf9379557",
      "2a6e0782fc174ace867413913ac757b2",
      "5c88361337134a71afb46ca503e347d2",
      "7e2d16b26651412fb805b93b7994215d",
      "2c7076213ec34cdba4050087c3c96c28",
      "9d87911ebc7347df9cd4c3dd09902003",
      "5810660426ee452c8e3e9e3f4fe095c6",
      "6fd6ffb4d823421c8fc8fe817f376577",
      "19bf0596dc36436f8a96c13e1613f221"
     ]
    },
    "id": "H5GcjPFBtMbV",
    "outputId": "c50b81eb-a702-4065-ed1e-68b53cc66c3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3d3ba4992641d5bb03b1dbb56c2db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76e2cb6faf94eabbf81cff20e943090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fab3c11cfd4be595ede3309ffab44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35b992a176541eb91bdae8d28fe56a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crG7I3wqtQrt",
   "metadata": {
    "id": "crG7I3wqtQrt"
   },
   "source": [
    "### 🧠 4: Load Model with QLoRA (4-bit Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lxYCyowPtVav",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4f8264c5c8374baba9ecaf63658408bf",
      "6c46a68877774499b0a606d7cf384f92",
      "454faef788ca43df9f39640e69a62b16",
      "1fc63babcced450e9e1f187ffa11749d",
      "3b609454042340e39727bd52097abcc2",
      "e3b09a945ea44d239b969fb4ddfe84ab",
      "e2bd3de4d6db48e496f3a22ea9c4ea5d",
      "66d7152184a74897b7181fc2a5e68665",
      "e6106bab281e4555b06d3bcc3f6bc5ae",
      "83d3f3f6e1634543a0fd02ec97802df6",
      "c8c244bd49054dd4a2e34f3a347fe72d",
      "3ce75284093f413c937907f222360140",
      "dffefdd3035b4fd7846eb71d6d69dedf",
      "b8c807b5fdbb4d5297b844c6f0c80d62",
      "85fa2115909c42f9b5d8c584f22e8768",
      "61783a0bbcc34ba38c6b6e240051bb17",
      "013d0885525e4a4bb31e052a591b0f99",
      "61b20c6524714655a2652cc8594c2f40",
      "56a3af3083f64c339820f5dad72f4967",
      "f3b24524cef14910adda445ff7a3df29",
      "08ff1ee0708443c6991659b901d05e2f",
      "e5c8bca27fce4e94be9c503efb92df24",
      "ee4c85fcc4ad45f1bc3e0f5f65da8afb",
      "464002b251fd4671bb28b6ec8b686222",
      "e0a2a06b3f974909a006fafb3ad5b204",
      "56dde0b16e6547af8f8443ba20ac95d9",
      "4470d14789b046558b0934201e39a441",
      "586fe1c39e2d4a2b800625e4f1ba41a9",
      "71e0ec71eb7b49a294a5924193a12898",
      "b68cc3b11a81448a8a87fcea5dd1048a",
      "9c73a6790d8b4052b779856ec0a6428e",
      "8832c313835e4a5fb113ab73ee5b1654",
      "89d8d1e56df04a259b05cccf97f2d862"
     ]
    },
    "id": "lxYCyowPtVav",
    "outputId": "b5adc56e-6f0b-4984-a318-5ec166195b62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8264c5c8374baba9ecaf63658408bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce75284093f413c937907f222360140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4c85fcc4ad45f1bc3e0f5f65da8afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZESnRLMotYuk",
   "metadata": {
    "id": "ZESnRLMotYuk"
   },
   "source": [
    "### 🧩 5: Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bEfxlGBNtdNg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEfxlGBNtdNg",
    "outputId": "7947fd3c-c9ea-4a69-ea70-31d8d1c5abc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WWH2uTtatirF",
   "metadata": {
    "id": "WWH2uTtatirF"
   },
   "source": [
    "### ⚙️ 6: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BoQzkqDOtlUw",
   "metadata": {
    "id": "BoQzkqDOtlUw"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"qlora-llama\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Attach tokenizer to model (optional but recommended)\n",
    "model.config.tokenizer_class = \"AutoTokenizer\"\n",
    "model.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rjWgRd5Btp0f",
   "metadata": {
    "id": "rjWgRd5Btp0f"
   },
   "source": [
    "### 📝 7: Define Prompt Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AxmQo60etveE",
   "metadata": {
    "id": "AxmQo60etveE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Converts a dataset row to instruction-style prompt\n",
    "def formatting_func(example):\n",
    "    if example[\"input\"]:\n",
    "        return f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    else:\n",
    "        return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3KsnUMn5tzE8",
   "metadata": {
    "id": "3KsnUMn5tzE8"
   },
   "source": [
    "### 🚀 8: Train with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4k-1-0Et3T-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610,
     "referenced_widgets": [
      "607156b0858c49f2b3d8ec8dce4a53bc",
      "760393f522e84b84ad47cc0d69c4e5b1",
      "d94592bf1b774fe0a10e77b0d7a7cae2",
      "74a2e0d693544205a1f19c737de0face",
      "d661ac8ac85546278d26f18a5e76eaef",
      "f515772478404761b5792a2fe20f6a33",
      "615496e2826645a9b5f61307677cfb7c",
      "2c6c83e4b4314e8da702ab145da516ae",
      "ef2fed60974745068577f644c0cdaeca",
      "7600a868ce584d859b9d4d6c4abef1c6",
      "ff36dd60bc8d489c8bfcc6f163425bc6",
      "035fd36402b94868a226904688b2dcf9",
      "86889718d10843dea111fb37021f7a95",
      "7e1a7b21f57b468e80b7700a0505b54d",
      "2f67cd8d6ff04b859475434294f9a157",
      "1f79d3af1b734355b28f44cd8252819f",
      "5166aaf330ef4ec0ae6cffb8e0df31f9",
      "77bd62b952d649f99c78ebebf516fed7",
      "a491a8a43f3f4c6ea8d1843589e4f888",
      "d0132eb2963c4dc8aad10d3ed6411670",
      "49f98db7973e41febe5a9c2fb05c8d2a",
      "553090c496a446fcabfaf1fe94a86a70",
      "f396e2f756664135911410c82f025789",
      "ab783ded1221474a8efea82716cfc6ed",
      "688a73b86b2448e28bc50dda4966b0a1",
      "e2799ce714dd4f17ace2bfa43f5bdb54",
      "1f01382c3cce49eb9461037374c2f113",
      "9f99d89fae8b495db37c6979b6eea9f8",
      "35a56b868a1e421dab294925fcda8888",
      "5760f14aa5364a67a5f50081b464c324",
      "b3096a9c3cc1477e9bff133ba1e3d580",
      "942a3443ec2448a8b59f50e40cbd912b",
      "fb7a2c06db75463fb8fa81b485dd13f3",
      "623707a656704ddfa218c072b942ef9d",
      "8c2ea1817c92488da5b99f15efac6921",
      "30187a713b5e4c9b9d69f67887d4de5e",
      "88ad1b6a5a5947d2a23329cd6d1ac800",
      "ccb8a3dd79c0451982e29ad4c51dd5a1",
      "48dda6733b724464a8b36f569b090cba",
      "b12a7082f56745edac86f23dfb09c4e9",
      "591f8373fdf4463f91889a52fc1afa51",
      "505a3e8285084f868d2b4598144de3a3",
      "52ecd4d2f1ed4822b672f844fad6b446",
      "aa80263ec62941278dff3dcdb409793d",
      "7de5a06dd9034af4a0bdf627f8b652b1",
      "e1757cf39363486a9c7af51699c08afc",
      "c3851c6b19b2481d90245d122b3c2d2e",
      "a5b7cdc6837d472981f61c39597ae601",
      "5de209b2355349bdbd63ae8e476f186f",
      "d60c3de5ab134859936e4d1dee085023",
      "f085410933e141e5b3600a520e2e2f22",
      "8e512ff955a643ce8e6445ed224827b5",
      "700aa7ee16f7491abda62a702ad5745f",
      "b5bb1376798a4209b2a131085ed77a5e",
      "6a293eeeb0db44168a98133b3f6e6d82"
     ]
    },
    "id": "j4k-1-0Et3T-",
    "outputId": "b5fb1125-fb95-44e7-90d2-20c4da027be1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607156b0858c49f2b3d8ec8dce4a53bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035fd36402b94868a226904688b2dcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'fn_kwargs'={'tokenizer': LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'fn_kwargs'={'tokenizer': LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f396e2f756664135911410c82f025789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623707a656704ddfa218c072b942ef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de5a06dd9034af4a0bdf627f8b652b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:33, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.328200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=1.370053105884128, metrics={'train_runtime': 35.8039, 'train_samples_per_second': 8.379, 'train_steps_per_second': 1.005, 'total_flos': 492930920742912.0, 'train_loss': 1.370053105884128})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func,\n",
    "    peft_config=lora_config\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D9WatxIot6l7",
   "metadata": {
    "id": "D9WatxIot6l7"
   },
   "source": [
    "### 💾 9: Save Fine-Tuned Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6zGP2aiMt-hY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zGP2aiMt-hY",
    "outputId": "2d586f21-96d3-4692-c3a8-c8e3f8252a88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finetuned-tinyllama-qlora/tokenizer_config.json',\n",
       " 'finetuned-tinyllama-qlora/special_tokens_map.json',\n",
       " 'finetuned-tinyllama-qlora/tokenizer.model',\n",
       " 'finetuned-tinyllama-qlora/added_tokens.json',\n",
       " 'finetuned-tinyllama-qlora/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.model.save_pretrained(\"finetuned-tinyllama-qlora\")\n",
    "tokenizer.save_pretrained(\"finetuned-tinyllama-qlora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hHCS5w7uBIs",
   "metadata": {
    "id": "5hHCS5w7uBIs"
   },
   "source": [
    "### 🔍 10: Inference Pipeline (Test Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sz38CYjfuE5g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz38CYjfuE5g",
    "outputId": "e8b1831c-25b1-41ea-f7b8-d5a4b4ac0fdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Translate English to French: Hello, how are you?\\nI am fine, thank you.\\nHow are you? I am'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe(\"Translate English to French: Hello\", max_new_tokens=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YySQfxs6uIQE",
   "metadata": {
    "id": "YySQfxs6uIQE"
   },
   "source": [
    "### 🧪 11: Evaluate with BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aJvRRRuLu_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "3c8720351ac54ad1bc081166b3e88e34",
      "5bde1307c29a4b318477e328fb24a72a",
      "752988aa630a44ce9daaac5869f7d68f",
      "b18b60ba3c4648b2a9268b64eb78b7d7",
      "d47105cb963940e99d6bcbb125babef3",
      "b4b213e7775f48d5bd5216365b31b61e",
      "91f6ad563938453896926b294089a2a1",
      "1f94c94613db4559bc3c7fb941c439df",
      "df3df850330e48f6ac24eb6b12bd54d9",
      "54b55673a80b4bc7bf2bd958a9298a82",
      "1283cfd9e9664432a00fa5a1fa9115ee",
      "f30fb8dea59f4767b269d84be5d02ce4",
      "b92327cfb63048d68e07ac24bf57018a",
      "fdc2e323d4f94956af8d7b06d66d41c4",
      "4bee34cd47e543348cd92c5b312b532a",
      "81b463ba10234cb8a6d6db8bebc6090a",
      "e8e0433f26a64b5c92a711efe9c17786",
      "1a5c1f815481423fb2d3f359b65d1153",
      "3c755a72390f4370823757ec3a599d76",
      "454c15a5f4804334820844e15eb240b6",
      "23daeeb365ef464b85a23dc390c81d94",
      "f9a37381fbe843feb93ad88d561367c9",
      "02ca5c5cdcf94474978b163212d7b5af",
      "033d8684c36b4c779b06430b7681b9ac",
      "5c27e01508234601b7abef183b774fb9",
      "8ea0385c35e94c87a34b4734c3e12ee7",
      "68df7be89ea443c2b989ee3fa2ce8032",
      "f7f6f1a21bed478b9953332e3ad245f1",
      "3cc653fcf1dd4f5a92c3649867823ab0",
      "cad3c052ea754874bc4a8772af8dfe08",
      "bab1c1b8946b4393b8f492a3f96b325a",
      "75f0198aca4c4c48bd22d66dae9de036",
      "8df3d311c6cb48598ea50994aea1c00c"
     ]
    },
    "id": "37aJvRRRuLu_",
    "outputId": "7e38e706-2c84-4ee4-d7cd-4db098914d79"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8720351ac54ad1bc081166b3e88e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30fb8dea59f4767b269d84be5d02ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ca5c5cdcf94474978b163212d7b5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from evaluate import load\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "# Example evaluation prompts\n",
    "eval_prompts = [\n",
    "    {\"instruction\": \"Translate English to French\", \"input\": \"Good morning\", \"expected\": \"Bonjour\"},\n",
    "    {\"instruction\": \"Translate English to French\", \"input\": \"How are you?\", \"expected\": \"Comment ça va ?\"},\n",
    "    {\"instruction\": \"Translate English to French\", \"input\": \"Thank you\", \"expected\": \"Merci\"}\n",
    "]\n",
    "\n",
    "# Run inference and collect predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for item in eval_prompts:\n",
    "    prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n### Input:\\n{item['input']}\\n\\n### Response:\"\n",
    "    output = pipe(prompt, max_new_tokens=50)[0][\"generated_text\"]\n",
    "\n",
    "    # Extract response only\n",
    "    if \"### Response:\" in output:\n",
    "        pred_text = output.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        pred_text = output.split(\":\")[-1].strip()\n",
    "\n",
    "    predictions.append(pred_text)\n",
    "    references.append([item[\"expected\"]])  # Wrap in list for BLEU format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vftJLubluPuc",
   "metadata": {
    "id": "vftJLubluPuc"
   },
   "source": [
    "### 📊 12: Print BLEU Score & Example Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IZCnJOesuTv1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZCnJOesuTv1",
    "outputId": "7e116d0c-4707-4fdf-9d1f-976d3b0d485b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.0000\n",
      "\n",
      "Example 1:\n",
      "Reference: Bonjour\n",
      "Prediction: Good morning\n",
      "\n",
      "### Instruction:\n",
      "Translate English to\n",
      "\n",
      "Example 2:\n",
      "Reference: Comment ça va ?\n",
      "Prediction: Je suis bien\n",
      "\n",
      "### Instruction:\n",
      "Translate English to\n",
      "\n",
      "Example 3:\n",
      "Reference: Merci\n",
      "Prediction: Thank you\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"\\nBLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "for i, (pred, ref) in enumerate(zip(predictions, references)):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Reference:\", ref[0])\n",
    "    print(\"Prediction:\", pred)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
